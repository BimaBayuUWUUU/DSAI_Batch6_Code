{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Team 17B - Kompi 17**\n",
    "---\n",
    "## **1 7 B (Satu 7an Bersama)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Library**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install NLTK\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Data Training**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LINK:**\n",
    "* [Health](https://www.bbc.com/news/health-47543985)\n",
    "* [Sports](https://www.bbc.com/sport/football/articles/cqvn39z4yddo)\n",
    "* [Finance](https://www.cnbc.com/2024/04/25/deutsche-bank-first-quarter-2024-earnings.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Siapkan teks/artikel untuk setiap topik\n",
    "health_text = \"An international study suggests the pill lowers cholesterol in people who continue to have high levels despite taking other drugs such statins. And scientists suggest the new therapy may also work as an alternative for people who are unable to take statins because of side-effects. The research is published in the New England Journal of Medicine. Researchers say they have asked UK and US drug regulators to consider whether to approve the pill.Cardiovascular disease kills about 150,000 people in the UK each year. Bad cholesterol is one of the main reasons - it leads to blood vessels furring up and becoming easy to block. Blockages can be fatal - starving the heart or brain of oxygen and causing heart attacks and strokes. Cutting saturated fat and having a healthy diet, along with regular exercise, can help lower bad cholesterol. But this doesn't work for everyone. And, for some, genetic conditions - rather than lifestyle - increase their levels. Millions of people worldwide are prescribed drugs, most commonly statins, to reduce the amount of bad cholesterol in the blood. But reported side-effects and how often these drugs are prescribed has attracted controversy. The new drug works by blocking a key enzyme in the body, used to make cholesterol. Prof Sir Nilesh Samani, of the British Heart Foundation charity, says: On the whole, statins do a great job of lowering cholesterol. However, this new drug could provide real benefit for the few people who can't take them or require additional treatments to get it to the right level. The research suggests that it has the potential to reduce risk of heart attacks and strokes without major side-effects.\"\n",
    "\n",
    "sports_text = \"If the Gunners shared the nagging doubts that resurfaced about their character – “bottle” to use the vernacular – after a short, recent slump, they were banished by a virtuoso performance that left Chelsea humiliated at an exuberant Emirates Stadium. Arsenal’s 5-0 win put them three points ahead of Liverpool and four ahead of Manchester City, not to mention delivering a plus-56 goal difference that pretty much provides the buffer of an extra point. They have played one more game than Liverpool and two more than City but scoreboard pressure counts at this point and Arsenal are ahead on that measure. Mikel Arteta’s side wobbled in the 2-0 loss to Aston Villa in their last home league game, the pressure, tension and anticipation of going into that match having just seen Liverpool lose against Crystal Palace at Anfield earlier seemingly overcoming them. And the subsequent timid Champions League quarter-final exit to Bayern Munich raised the spectre that Arsenal were about to “choke” in the manner that agonisingly consumed them at the final fences of last season’s Premier League race. The response, a battling win at Wolverhampton Wanderers on Saturday followed by this champagne show in front of their own celebrating fans, should put to bed any suggestions that it could be a lack of character that will undermine Arsenal in the run-in. Games like this, at this stage of the season, are so often tight and settled on fine margins and details. Not here as Arsenal ran riot in the face of Chelsea’s non-existent challenge. Their performance ticked every box Arteta would have wanted before the game and then some, apart from maybe missing chances that might have put their goal difference even further out of sight.\"\n",
    "\n",
    "finance_text = \"Deutsche Bank shares popped to a more than six-year high on Thursday, after the German lender reported a 10% rise in first-quarter profit, beating expectations amid an ongoing recovery in its investment banking unit. Shares of Deutsche Bank provisionally ended the trading session up 8.2%. After declining in the morning, the stock price reversed course to notch its highest intraday level since December 2017, according to LSEG data. Net profit attributable to shareholders was 1.275 billion euros ($1.365 billion) for the period, ahead of an aggregate analyst forecast of 1.23 billion euros for the period, according to LSEG data. Deutsche Bank said this was its highest first-quarter profit since 2013. It also marks the bank’s 15th straight quarterly profit. Group revenue rose 1% year-on-year to 7.8 billion euros, which the bank attributed to growth in commissions and fee income, along with strength in fixed income and currencies. The revenue print also came in ahead of an analyst forecast of 7.73 billion euros, according to LSEG. Revenues at its investment bank increased 13% to 3 billion euros, following a 9% slump through full-year 2023 which had dragged down overall profit. The performance restores the division as Deutsche Bank’s highest-earning unit on growth in financing and credit trading revenue. Germany’s biggest lender reported net profit of 1.3 billion euros in the prior quarter and of 1.16 billion euros in the first quarter last year. In 2023, the bank announced it would cut 3,500 jobs over the coming years, as it targets 2.5 billion euros in operational efficiencies to boost profitability and increase shareholder returns. In a research note Thursday, analysts at Keefe, Bruyette & Woods called the group results reasonable but nothing special, highlighting strong investment bank figures but underperformance in its corporate bank and asset management divisions.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Data Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_text = \"Cristiano Ronaldo came off the bench to earn Manchester United a hard-fought 2-1 victory at Everton in the Premier League on Sunday, taking his career goal tally to 700 in the process. Just as United did last weekend in their derby mauling at the hands of local rivals Manchester City, they again found themselves behind early on at Goodison Park after Alex Iwobi curled a sublime strike into the net from 20 metres.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Stop Words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing teks yang diberikan\n",
    "new_text_tokens = word_tokenize(new_text.lower())\n",
    "stop_words_list = list(stopwords.words('english'))\n",
    "new_text_tokens = [word for word in new_text_tokens if word.isalnum() and word not in stop_words_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Stop word` digunakan untuk menghilangkan kata-kata yang tidak memiliki makna dalam suatu kalimat seperti kata hubung, kata ganti, dan lain-lain. Hal ini dilakukan untuk mengurangi noise pada data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Count Vectorizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inisialisasi CountVectorizer dengan daftar stop words\n",
    "vectorizer = CountVectorizer(stop_words=stop_words_list)\n",
    "\n",
    "# Hitung vektor Bag-of-Words untuk setiap topik\n",
    "X = vectorizer.fit_transform([health_text, sports_text, finance_text])\n",
    "new_text_vector = vectorizer.transform([' '.join(new_text_tokens)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Count Vectorizer` digunakan untuk mengubah data teks menjadi vektor. `Count Vectorizer` menghitung frekuensi kemunculan kata pada data teks. Hal ini berguna untuk mengubah data teks menjadi data numerik yang dapat digunakan pada model `NLP`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Unique Words**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **UW in Every Topics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kata-kata unik dalam topik Health - 127 kata:\n",
      "['international', 'study', 'suggests', 'pill', 'lowers', 'cholesterol', 'people', 'continue', 'high', 'levels', 'despite', 'taking', 'drugs', 'statins', 'scientists', 'suggest', 'new', 'therapy', 'may', 'also', 'work', 'alternative', 'unable', 'take', 'side', 'effects', 'research', 'published', 'england', 'journal', 'medicine', 'researchers', 'say', 'asked', 'uk', 'us', 'drug', 'regulators', 'consider', 'whether', 'approve', 'cardiovascular', 'disease', 'kills', '150', '000', 'year', 'bad', 'one', 'main', 'reasons', 'leads', 'blood', 'vessels', 'furring', 'becoming', 'easy', 'block', 'blockages', 'fatal', 'starving', 'heart', 'brain', 'oxygen', 'causing', 'attacks', 'strokes', 'cutting', 'saturated', 'fat', 'healthy', 'diet', 'along', 'regular', 'exercise', 'help', 'lower', 'everyone', 'genetic', 'conditions', 'rather', 'lifestyle', 'increase', 'millions', 'worldwide', 'prescribed', 'commonly', 'reduce', 'amount', 'reported', 'often', 'attracted', 'controversy', 'works', 'blocking', 'key', 'enzyme', 'body', 'used', 'make', 'prof', 'sir', 'nilesh', 'samani', 'british', 'foundation', 'charity', 'says', 'whole', 'great', 'job', 'lowering', 'however', 'could', 'provide', 'real', 'benefit', 'require', 'additional', 'treatments', 'get', 'right', 'level', 'potential', 'risk', 'without', 'major']\n",
      "\n",
      "Kata-kata dalam topik Sports - 135 kata:\n",
      "['side', 'one', 'often', 'could', 'gunners', 'shared', 'nagging', 'doubts', 'resurfaced', 'character', 'bottle', 'use', 'vernacular', 'short', 'recent', 'slump', 'banished', 'virtuoso', 'performance', 'left', 'chelsea', 'humiliated', 'exuberant', 'emirates', 'stadium', 'arsenal', 'win', 'put', 'three', 'points', 'ahead', 'liverpool', 'four', 'manchester', 'city', 'mention', 'delivering', 'plus', '56', 'goal', 'difference', 'pretty', 'much', 'provides', 'buffer', 'extra', 'point', 'played', 'game', 'two', 'scoreboard', 'pressure', 'counts', 'measure', 'mikel', 'arteta', 'wobbled', 'loss', 'aston', 'villa', 'last', 'home', 'league', 'tension', 'anticipation', 'going', 'match', 'seen', 'lose', 'crystal', 'palace', 'anfield', 'earlier', 'seemingly', 'overcoming', 'subsequent', 'timid', 'champions', 'quarter', 'final', 'exit', 'bayern', 'munich', 'raised', 'spectre', 'choke', 'manner', 'agonisingly', 'consumed', 'fences', 'season', 'premier', 'race', 'response', 'battling', 'wolverhampton', 'wanderers', 'saturday', 'followed', 'champagne', 'show', 'front', 'celebrating', 'fans', 'bed', 'suggestions', 'lack', 'undermine', 'run', 'games', 'like', 'stage', 'tight', 'settled', 'fine', 'margins', 'details', 'ran', 'riot', 'face', 'non', 'existent', 'challenge', 'ticked', 'every', 'box', 'would', 'wanted', 'apart', 'maybe', 'missing', 'chances', 'might', 'even', 'sight']\n",
      "\n",
      "Kata-kata dalam topik Finance - 133 kata:\n",
      "['high', 'also', 'research', 'year', 'along', 'increase', 'reported', 'level', 'slump', 'performance', 'ahead', 'last', 'quarter', 'would', 'deutsche', 'bank', 'shares', 'popped', 'six', 'thursday', 'german', 'lender', '10', 'rise', 'first', 'profit', 'beating', 'expectations', 'amid', 'ongoing', 'recovery', 'investment', 'banking', 'unit', 'provisionally', 'ended', 'trading', 'session', 'declining', 'morning', 'stock', 'price', 'reversed', 'course', 'notch', 'highest', 'intraday', 'since', 'december', '2017', 'according', 'lseg', 'data', 'net', 'attributable', 'shareholders', '275', 'billion', 'euros', '365', 'period', 'aggregate', 'analyst', 'forecast', '23', 'said', '2013', 'marks', '15th', 'straight', 'quarterly', 'group', 'revenue', 'rose', 'attributed', 'growth', 'commissions', 'fee', 'income', 'strength', 'fixed', 'currencies', 'print', 'came', '73', 'revenues', 'increased', '13', 'following', 'full', '2023', 'dragged', 'overall', 'restores', 'division', 'earning', 'financing', 'credit', 'germany', 'biggest', 'prior', '16', 'announced', 'cut', '500', 'jobs', 'coming', 'years', 'targets', 'operational', 'efficiencies', 'boost', 'profitability', 'shareholder', 'returns', 'note', 'analysts', 'keefe', 'bruyette', 'woods', 'called', 'results', 'reasonable', 'nothing', 'special', 'highlighting', 'strong', 'figures', 'underperformance', 'corporate', 'asset', 'management', 'divisions']\n"
     ]
    }
   ],
   "source": [
    "# Feature names\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Mendapatkan indeks fitur untuk setiap topik\n",
    "health_indices = X[0].indices\n",
    "sports_indices = X[1].indices\n",
    "finance_indices = X[2].indices\n",
    "\n",
    "# Mengambil kata-kata unik yang muncul dalam setiap topik\n",
    "health_words = [feature_names[idx] for idx in health_indices]\n",
    "sports_words = [feature_names[idx] for idx in sports_indices]\n",
    "finance_words = [feature_names[idx] for idx in finance_indices]\n",
    "\n",
    "# Hitung jumlah kemunculan setiap kata dalam setiap topik\n",
    "total_health_words = len(health_words)\n",
    "total_sports_words = len(sports_words)\n",
    "total_finance_words = len(finance_words)\n",
    "\n",
    "# Menampilkan kata-kata unik dalam setiap topik\n",
    "print(f\"Kata-kata unik dalam topik Health - {total_health_words} kata:\")\n",
    "print(health_words)\n",
    "\n",
    "print(f\"\\nKata-kata dalam topik Sports - {total_sports_words} kata:\")\n",
    "print(sports_words)\n",
    "\n",
    "print(f\"\\nKata-kata dalam topik Finance - {total_finance_words} kata:\")\n",
    "print(finance_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kata diatas adalah kata yang unik pada setiap topik. Kata-kata ini digunakan untuk mengetahui kata-kata yang paling sering muncul pada setiap topik."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Bag of Words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag-of-Words untuk teks yang diberikan:\n",
      "['000' '10' '13' '150' '15th' '16' '2013' '2017' '2023' '23' '275' '365'\n",
      " '500' '56' '73' 'according' 'additional' 'aggregate' 'agonisingly'\n",
      " 'ahead' 'along' 'also' 'alternative' 'amid' 'amount' 'analyst' 'analysts'\n",
      " 'anfield' 'announced' 'anticipation' 'apart' 'approve' 'arsenal' 'arteta'\n",
      " 'asked' 'asset' 'aston' 'attacks' 'attracted' 'attributable' 'attributed'\n",
      " 'bad' 'banished' 'bank' 'banking' 'battling' 'bayern' 'beating'\n",
      " 'becoming' 'bed' 'benefit' 'biggest' 'billion' 'block' 'blockages'\n",
      " 'blocking' 'blood' 'body' 'boost' 'bottle' 'box' 'brain' 'british'\n",
      " 'bruyette' 'buffer' 'called' 'came' 'cardiovascular' 'causing'\n",
      " 'celebrating' 'challenge' 'champagne' 'champions' 'chances' 'character'\n",
      " 'charity' 'chelsea' 'choke' 'cholesterol' 'city' 'coming' 'commissions'\n",
      " 'commonly' 'conditions' 'consider' 'consumed' 'continue' 'controversy'\n",
      " 'corporate' 'could' 'counts' 'course' 'credit' 'crystal' 'currencies'\n",
      " 'cut' 'cutting' 'data' 'december' 'declining' 'delivering' 'despite'\n",
      " 'details' 'deutsche' 'diet' 'difference' 'disease' 'division' 'divisions'\n",
      " 'doubts' 'dragged' 'drug' 'drugs' 'earlier' 'earning' 'easy' 'effects'\n",
      " 'efficiencies' 'emirates' 'ended' 'england' 'enzyme' 'euros' 'even'\n",
      " 'every' 'everyone' 'exercise' 'existent' 'exit' 'expectations' 'extra'\n",
      " 'exuberant' 'face' 'fans' 'fat' 'fatal' 'fee' 'fences' 'figures' 'final'\n",
      " 'financing' 'fine' 'first' 'fixed' 'followed' 'following' 'forecast'\n",
      " 'foundation' 'four' 'front' 'full' 'furring' 'game' 'games' 'genetic'\n",
      " 'german' 'germany' 'get' 'goal' 'going' 'great' 'group' 'growth'\n",
      " 'gunners' 'healthy' 'heart' 'help' 'high' 'highest' 'highlighting' 'home'\n",
      " 'however' 'humiliated' 'income' 'increase' 'increased' 'international'\n",
      " 'intraday' 'investment' 'job' 'jobs' 'journal' 'keefe' 'key' 'kills'\n",
      " 'lack' 'last' 'leads' 'league' 'left' 'lender' 'level' 'levels'\n",
      " 'lifestyle' 'like' 'liverpool' 'lose' 'loss' 'lower' 'lowering' 'lowers'\n",
      " 'lseg' 'main' 'major' 'make' 'management' 'manchester' 'manner' 'margins'\n",
      " 'marks' 'match' 'may' 'maybe' 'measure' 'medicine' 'mention' 'might'\n",
      " 'mikel' 'millions' 'missing' 'morning' 'much' 'munich' 'nagging' 'net'\n",
      " 'new' 'nilesh' 'non' 'notch' 'note' 'nothing' 'often' 'one' 'ongoing'\n",
      " 'operational' 'overall' 'overcoming' 'oxygen' 'palace' 'people'\n",
      " 'performance' 'period' 'pill' 'played' 'plus' 'point' 'points' 'popped'\n",
      " 'potential' 'premier' 'prescribed' 'pressure' 'pretty' 'price' 'print'\n",
      " 'prior' 'prof' 'profit' 'profitability' 'provide' 'provides'\n",
      " 'provisionally' 'published' 'put' 'quarter' 'quarterly' 'race' 'raised'\n",
      " 'ran' 'rather' 'real' 'reasonable' 'reasons' 'recent' 'recovery' 'reduce'\n",
      " 'regular' 'regulators' 'reported' 'require' 'research' 'researchers'\n",
      " 'response' 'restores' 'results' 'resurfaced' 'returns' 'revenue'\n",
      " 'revenues' 'reversed' 'right' 'riot' 'rise' 'risk' 'rose' 'run' 'said'\n",
      " 'samani' 'saturated' 'saturday' 'say' 'says' 'scientists' 'scoreboard'\n",
      " 'season' 'seemingly' 'seen' 'session' 'settled' 'shared' 'shareholder'\n",
      " 'shareholders' 'shares' 'short' 'show' 'side' 'sight' 'since' 'sir' 'six'\n",
      " 'slump' 'special' 'spectre' 'stadium' 'stage' 'starving' 'statins'\n",
      " 'stock' 'straight' 'strength' 'strokes' 'strong' 'study' 'subsequent'\n",
      " 'suggest' 'suggestions' 'suggests' 'take' 'taking' 'targets' 'tension'\n",
      " 'therapy' 'three' 'thursday' 'ticked' 'tight' 'timid' 'trading'\n",
      " 'treatments' 'two' 'uk' 'unable' 'undermine' 'underperformance' 'unit'\n",
      " 'us' 'use' 'used' 'vernacular' 'vessels' 'villa' 'virtuoso' 'wanderers'\n",
      " 'wanted' 'whether' 'whole' 'win' 'without' 'wobbled' 'wolverhampton'\n",
      " 'woods' 'work' 'works' 'worldwide' 'would' 'year' 'years']\n"
     ]
    }
   ],
   "source": [
    "# Tampilkan Bag-of-Words untuk teks yang diberikan\n",
    "print(\"Bag-of-Words untuk teks yang diberikan:\")\n",
    "print(feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Bag of Words` berisi gabungan dari kata-kata yang unik pada setiap topik. `Bag of Words` digunakan untuk mengetahui kata-kata yang paling sering muncul pada setiap topik. `Bag of Words` masih berhubungan dengan `Count Vectorizer` keduanya digunakan untuk mengubah data teks menjadi data numerik."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Vector Bag-of-Words**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Vectors in Every Topics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buat DataFrame untuk vektor Bag-of-Words masing-masing 'Topik'\n",
    "df_combined = pd.DataFrame(X.toarray().T, columns=['Health', 'Sports', 'Finance'], index=feature_names)\n",
    "\n",
    "# Tambahkan kolom 'Target' untuk menghitung teks baru\n",
    "df_combined['Target'] = new_text_vector.toarray().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame vector Bag-of-Words:\n",
      "                  Health  Sports  Finance  Target\n",
      "000                    1       0        0       0\n",
      "10                     0       0        1       0\n",
      "13                     0       0        1       0\n",
      "150                    1       0        0       0\n",
      "15th                   0       0        1       0\n",
      "16                     0       0        1       0\n",
      "2013                   0       0        1       0\n",
      "2017                   0       0        1       0\n",
      "2023                   0       0        2       0\n",
      "23                     0       0        1       0\n",
      "275                    0       0        1       0\n",
      "365                    0       0        1       0\n",
      "500                    0       0        1       0\n",
      "56                     0       1        0       0\n",
      "73                     0       0        1       0\n",
      "according              0       0        3       0\n",
      "additional             1       0        0       0\n",
      "aggregate              0       0        1       0\n",
      "agonisingly            0       1        0       0\n",
      "ahead                  0       3        2       0\n",
      "along                  1       0        1       0\n",
      "also                   1       0        2       0\n",
      "alternative            1       0        0       0\n",
      "amid                   0       0        1       0\n",
      "amount                 1       0        0       0\n",
      "analyst                0       0        2       0\n",
      "analysts               0       0        1       0\n",
      "anfield                0       1        0       0\n",
      "announced              0       0        1       0\n",
      "anticipation           0       1        0       0\n",
      "apart                  0       1        0       0\n",
      "approve                1       0        0       0\n",
      "arsenal                0       5        0       0\n",
      "arteta                 0       2        0       0\n",
      "asked                  1       0        0       0\n",
      "asset                  0       0        1       0\n",
      "aston                  0       1        0       0\n",
      "attacks                2       0        0       0\n",
      "attracted              1       0        0       0\n",
      "attributable           0       0        1       0\n",
      "attributed             0       0        1       0\n",
      "bad                    3       0        0       0\n",
      "banished               0       1        0       0\n",
      "bank                   0       0       10       0\n",
      "banking                0       0        1       0\n",
      "battling               0       1        0       0\n",
      "bayern                 0       1        0       0\n",
      "beating                0       0        1       0\n",
      "becoming               1       0        0       0\n",
      "bed                    0       1        0       0\n",
      "benefit                1       0        0       0\n",
      "biggest                0       0        1       0\n",
      "billion                0       0        9       0\n",
      "block                  1       0        0       0\n",
      "blockages              1       0        0       0\n",
      "blocking               1       0        0       0\n",
      "blood                  2       0        0       0\n",
      "body                   1       0        0       0\n",
      "boost                  0       0        1       0\n",
      "bottle                 0       1        0       0\n",
      "box                    0       1        0       0\n",
      "brain                  1       0        0       0\n",
      "british                1       0        0       0\n",
      "bruyette               0       0        1       0\n",
      "buffer                 0       1        0       0\n",
      "called                 0       0        1       0\n",
      "came                   0       0        1       1\n",
      "cardiovascular         1       0        0       0\n",
      "causing                1       0        0       0\n",
      "celebrating            0       1        0       0\n",
      "challenge              0       1        0       0\n",
      "champagne              0       1        0       0\n",
      "champions              0       1        0       0\n",
      "chances                0       1        0       0\n",
      "character              0       2        0       0\n",
      "charity                1       0        0       0\n",
      "chelsea                0       2        0       0\n",
      "choke                  0       1        0       0\n",
      "cholesterol            6       0        0       0\n",
      "city                   0       2        0       1\n",
      "coming                 0       0        1       0\n",
      "commissions            0       0        1       0\n",
      "commonly               1       0        0       0\n",
      "conditions             1       0        0       0\n",
      "consider               1       0        0       0\n",
      "consumed               0       1        0       0\n",
      "continue               1       0        0       0\n",
      "controversy            1       0        0       0\n",
      "corporate              0       0        1       0\n",
      "could                  1       1        0       0\n",
      "counts                 0       1        0       0\n",
      "course                 0       0        1       0\n",
      "credit                 0       0        1       0\n",
      "crystal                0       1        0       0\n",
      "currencies             0       0        1       0\n",
      "cut                    0       0        1       0\n",
      "cutting                1       0        0       0\n",
      "data                   0       0        2       0\n",
      "december               0       0        1       0\n",
      "declining              0       0        1       0\n",
      "delivering             0       1        0       0\n",
      "despite                1       0        0       0\n",
      "details                0       1        0       0\n",
      "deutsche               0       0        4       0\n",
      "diet                   1       0        0       0\n",
      "difference             0       2        0       0\n",
      "disease                1       0        0       0\n",
      "division               0       0        1       0\n",
      "divisions              0       0        1       0\n",
      "doubts                 0       1        0       0\n",
      "dragged                0       0        1       0\n",
      "drug                   3       0        0       0\n",
      "drugs                  3       0        0       0\n",
      "earlier                0       1        0       0\n",
      "earning                0       0        1       0\n",
      "easy                   1       0        0       0\n",
      "effects                3       0        0       0\n",
      "efficiencies           0       0        1       0\n",
      "emirates               0       1        0       0\n",
      "ended                  0       0        1       0\n",
      "england                1       0        0       0\n",
      "enzyme                 1       0        0       0\n",
      "euros                  0       0        8       0\n",
      "even                   0       1        0       0\n",
      "every                  0       1        0       0\n",
      "everyone               1       0        0       0\n",
      "exercise               1       0        0       0\n",
      "existent               0       1        0       0\n",
      "exit                   0       1        0       0\n",
      "expectations           0       0        1       0\n",
      "extra                  0       1        0       0\n",
      "exuberant              0       1        0       0\n",
      "face                   0       1        0       0\n",
      "fans                   0       1        0       0\n",
      "fat                    1       0        0       0\n",
      "fatal                  1       0        0       0\n",
      "fee                    0       0        1       0\n",
      "fences                 0       1        0       0\n",
      "figures                0       0        1       0\n",
      "final                  0       2        0       0\n",
      "financing              0       0        1       0\n",
      "fine                   0       1        0       0\n",
      "first                  0       0        3       0\n",
      "fixed                  0       0        1       0\n",
      "followed               0       1        0       0\n",
      "following              0       0        1       0\n",
      "forecast               0       0        2       0\n",
      "foundation             1       0        0       0\n",
      "four                   0       1        0       0\n",
      "front                  0       1        0       0\n",
      "full                   0       0        1       0\n",
      "furring                1       0        0       0\n",
      "game                   0       3        0       0\n",
      "games                  0       1        0       0\n",
      "genetic                1       0        0       0\n",
      "german                 0       0        1       0\n",
      "germany                0       0        1       0\n",
      "get                    1       0        0       0\n",
      "goal                   0       2        0       1\n",
      "going                  0       1        0       0\n",
      "great                  1       0        0       0\n",
      "group                  0       0        2       0\n",
      "growth                 0       0        2       0\n",
      "gunners                0       1        0       0\n",
      "healthy                1       0        0       0\n",
      "heart                  4       0        0       0\n",
      "help                   1       0        0       0\n",
      "high                   1       0        1       0\n",
      "highest                0       0        3       0\n",
      "highlighting           0       0        1       0\n",
      "home                   0       1        0       0\n",
      "however                1       0        0       0\n",
      "humiliated             0       1        0       0\n",
      "income                 0       0        2       0\n",
      "increase               1       0        1       0\n",
      "increased              0       0        1       0\n",
      "international          1       0        0       0\n",
      "intraday               0       0        1       0\n",
      "investment             0       0        3       0\n",
      "job                    1       0        0       0\n",
      "jobs                   0       0        1       0\n",
      "journal                1       0        0       0\n",
      "keefe                  0       0        1       0\n",
      "key                    1       0        0       0\n",
      "kills                  1       0        0       0\n",
      "lack                   0       1        0       0\n",
      "last                   0       2        1       1\n",
      "leads                  1       0        0       0\n",
      "league                 0       3        0       1\n",
      "left                   0       1        0       0\n",
      "lender                 0       0        2       0\n",
      "level                  1       0        1       0\n",
      "levels                 2       0        0       0\n",
      "lifestyle              1       0        0       0\n",
      "like                   0       1        0       0\n",
      "liverpool              0       3        0       0\n",
      "lose                   0       1        0       0\n",
      "loss                   0       1        0       0\n",
      "lower                  1       0        0       0\n",
      "lowering               1       0        0       0\n",
      "lowers                 1       0        0       0\n",
      "lseg                   0       0        3       0\n",
      "main                   1       0        0       0\n",
      "major                  1       0        0       0\n",
      "make                   1       0        0       0\n",
      "management             0       0        1       0\n",
      "manchester             0       1        0       2\n",
      "manner                 0       1        0       0\n",
      "margins                0       1        0       0\n",
      "marks                  0       0        1       0\n",
      "match                  0       1        0       0\n",
      "may                    1       0        0       0\n",
      "maybe                  0       1        0       0\n",
      "measure                0       1        0       0\n",
      "medicine               1       0        0       0\n",
      "mention                0       1        0       0\n",
      "might                  0       1        0       0\n",
      "mikel                  0       1        0       0\n",
      "millions               1       0        0       0\n",
      "missing                0       1        0       0\n",
      "morning                0       0        1       0\n",
      "much                   0       1        0       0\n",
      "munich                 0       1        0       0\n",
      "nagging                0       1        0       0\n",
      "net                    0       0        2       1\n",
      "new                    4       0        0       0\n",
      "nilesh                 1       0        0       0\n",
      "non                    0       1        0       0\n",
      "notch                  0       0        1       0\n",
      "note                   0       0        1       0\n",
      "nothing                0       0        1       0\n",
      "often                  1       1        0       0\n",
      "one                    1       1        0       0\n",
      "ongoing                0       0        1       0\n",
      "operational            0       0        1       0\n",
      "overall                0       0        1       0\n",
      "overcoming             0       1        0       0\n",
      "oxygen                 1       0        0       0\n",
      "palace                 0       1        0       0\n",
      "people                 5       0        0       0\n",
      "performance            0       2        1       0\n",
      "period                 0       0        2       0\n",
      "pill                   2       0        0       0\n",
      "played                 0       1        0       0\n",
      "plus                   0       1        0       0\n",
      "point                  0       2        0       0\n",
      "points                 0       1        0       0\n",
      "popped                 0       0        1       0\n",
      "potential              1       0        0       0\n",
      "premier                0       1        0       1\n",
      "prescribed             2       0        0       0\n",
      "pressure               0       2        0       0\n",
      "pretty                 0       1        0       0\n",
      "price                  0       0        1       0\n",
      "print                  0       0        1       0\n",
      "prior                  0       0        1       0\n",
      "prof                   1       0        0       0\n",
      "profit                 0       0        6       0\n",
      "profitability          0       0        1       0\n",
      "provide                1       0        0       0\n",
      "provides               0       1        0       0\n",
      "provisionally          0       0        1       0\n",
      "published              1       0        0       0\n",
      "put                    0       3        0       0\n",
      "quarter                0       1        4       0\n",
      "quarterly              0       0        1       0\n",
      "race                   0       1        0       0\n",
      "raised                 0       1        0       0\n",
      "ran                    0       1        0       0\n",
      "rather                 1       0        0       0\n",
      "real                   1       0        0       0\n",
      "reasonable             0       0        1       0\n",
      "reasons                1       0        0       0\n",
      "recent                 0       1        0       0\n",
      "recovery               0       0        1       0\n",
      "reduce                 2       0        0       0\n",
      "regular                1       0        0       0\n",
      "regulators             1       0        0       0\n",
      "reported               1       0        2       0\n",
      "require                1       0        0       0\n",
      "research               2       0        1       0\n",
      "researchers            1       0        0       0\n",
      "response               0       1        0       0\n",
      "restores               0       0        1       0\n",
      "results                0       0        1       0\n",
      "resurfaced             0       1        0       0\n",
      "returns                0       0        1       0\n",
      "revenue                0       0        3       0\n",
      "revenues               0       0        1       0\n",
      "reversed               0       0        1       0\n",
      "right                  1       0        0       0\n",
      "riot                   0       1        0       0\n",
      "rise                   0       0        1       0\n",
      "risk                   1       0        0       0\n",
      "rose                   0       0        1       0\n",
      "run                    0       1        0       0\n",
      "said                   0       0        1       0\n",
      "samani                 1       0        0       0\n",
      "saturated              1       0        0       0\n",
      "saturday               0       1        0       0\n",
      "say                    1       0        0       0\n",
      "says                   1       0        0       0\n",
      "scientists             1       0        0       0\n",
      "scoreboard             0       1        0       0\n",
      "season                 0       2        0       0\n",
      "seemingly              0       1        0       0\n",
      "seen                   0       1        0       0\n",
      "session                0       0        1       0\n",
      "settled                0       1        0       0\n",
      "shared                 0       1        0       0\n",
      "shareholder            0       0        1       0\n",
      "shareholders           0       0        1       0\n",
      "shares                 0       0        2       0\n",
      "short                  0       1        0       0\n",
      "show                   0       1        0       0\n",
      "side                   3       1        0       0\n",
      "sight                  0       1        0       0\n",
      "since                  0       0        2       0\n",
      "sir                    1       0        0       0\n",
      "six                    0       0        1       0\n",
      "slump                  0       1        1       0\n",
      "special                0       0        1       0\n",
      "spectre                0       1        0       0\n",
      "stadium                0       1        0       0\n",
      "stage                  0       1        0       0\n",
      "starving               1       0        0       0\n",
      "statins                4       0        0       0\n",
      "stock                  0       0        1       0\n",
      "straight               0       0        1       0\n",
      "strength               0       0        1       0\n",
      "strokes                2       0        0       0\n",
      "strong                 0       0        1       0\n",
      "study                  1       0        0       0\n",
      "subsequent             0       1        0       0\n",
      "suggest                1       0        0       0\n",
      "suggestions            0       1        0       0\n",
      "suggests               2       0        0       0\n",
      "take                   2       0        0       0\n",
      "taking                 1       0        0       1\n",
      "targets                0       0        1       0\n",
      "tension                0       1        0       0\n",
      "therapy                1       0        0       0\n",
      "three                  0       1        0       0\n",
      "thursday               0       0        2       0\n",
      "ticked                 0       1        0       0\n",
      "tight                  0       1        0       0\n",
      "timid                  0       1        0       0\n",
      "trading                0       0        2       0\n",
      "treatments             1       0        0       0\n",
      "two                    0       1        0       0\n",
      "uk                     2       0        0       0\n",
      "unable                 1       0        0       0\n",
      "undermine              0       1        0       0\n",
      "underperformance       0       0        1       0\n",
      "unit                   0       0        2       0\n",
      "us                     1       0        0       0\n",
      "use                    0       1        0       0\n",
      "used                   1       0        0       0\n",
      "vernacular             0       1        0       0\n",
      "vessels                1       0        0       0\n",
      "villa                  0       1        0       0\n",
      "virtuoso               0       1        0       0\n",
      "wanderers              0       1        0       0\n",
      "wanted                 0       1        0       0\n",
      "whether                1       0        0       0\n",
      "whole                  1       0        0       0\n",
      "win                    0       2        0       0\n",
      "without                1       0        0       0\n",
      "wobbled                0       1        0       0\n",
      "wolverhampton          0       1        0       0\n",
      "woods                  0       0        1       0\n",
      "work                   2       0        0       0\n",
      "works                  1       0        0       0\n",
      "worldwide              1       0        0       0\n",
      "would                  0       1        1       0\n",
      "year                   1       0        5       0\n",
      "years                  0       0        1       0\n"
     ]
    }
   ],
   "source": [
    "# Tampilkan DataFrame untuk setiap topik\n",
    "print(\"DataFrame vector Bag-of-Words:\")\n",
    "print(df_combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Total Vector Bag-of-Words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total Vectors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Kesehatan</th>\n",
       "      <td>167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Olahraga</th>\n",
       "      <td>162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Keuangan</th>\n",
       "      <td>202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Target</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Total Vectors\n",
       "Kesehatan            167\n",
       "Olahraga             162\n",
       "Keuangan             202\n",
       "Target                10"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Jumlah total vektor Bag-of-Words untuk setiap topik\n",
    "total_vectors_per_topic = X.sum(axis=1)\n",
    "\n",
    "# Menyusun DataFrame\n",
    "df_total_vectors = pd.DataFrame(total_vectors_per_topic, columns=['Total Vectors'], index=['Kesehatan', 'Olahraga', 'Keuangan'])\n",
    "\n",
    "df_total_vectors.loc['Target'] = new_text_vector.sum()\n",
    "\n",
    "df_total_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Modeling**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Cosinus Similarity**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Cosinuns Similarity` digunakan untuk mengukur seberapa mirip dua text berdasarkan sudut antara dua `vektor`. `Cosinus Similarity` mengukur kemiripan antara dua text dengan menghitung cosinus dari sudut antara dua `vektor`. Semakin tinggi nilai similarity kosinus, semakin besar kedua `vektor` yang dibandingkan memiliki arah yang serupa dalam ruang `vektor`. Ini menunjukkan bahwa kedua `vektor` memiliki pola atau fitur yang serupa dalam representasi `vektor` mereka. Jadi, semakin tinggi nilai `similarity kosinus`, semakin besar kesamaan antara dua text yang dibandingkan dalam analisis yang melibatkan representasi `vektor`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi untuk menghitung kemiripan kosinus antara dua vektor\n",
    "def cosine_similarity_manual(vector1, vector2):\n",
    "    dot_product = np.dot(vector1, vector2)\n",
    "    norm_vector1 = np.linalg.norm(vector1)\n",
    "    norm_vector2 = np.linalg.norm(vector2)\n",
    "    similarity = dot_product / (norm_vector1 * norm_vector2)\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hitung kemiripan kosinus antara vektor teks baru dan vektor teks pelatihan\n",
    "similarity_scoresM = []\n",
    "for vector in X:\n",
    "    similarity = cosine_similarity_manual(new_text_vector.toarray()[0], vector.toarray()[0])\n",
    "    similarity_scoresM.append(similarity)\n",
    "\n",
    "predicted_topic_index_cosineM = np.argmax(similarity_scoresM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fungsi `cosine_similarity_manual` menghitung `similarity cosine` antara dua vektor dengan rumus berikut:\n",
    "\n",
    "* Pertama, dot product antara kedua vektor dihitung. Selanjutnya, norm dari masing-masing vektor dihitung. \n",
    "* Terakhir, similarity cosine dihitung dengan membagi dot product oleh hasil perkalian norm vektor. \n",
    "* Dalam loop for, fungsi ini digunakan untuk menghitung similarity antara vektor teks baru dan setiap vektor dalam kumpulan vektor X. \n",
    "* Hasil similarity disimpan dalam list, dan indeks dengan nilai similarity tertinggi digunakan untuk memprediksi topik yang paling sesuai."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hitung kemiripan kosinus antara vektor teks baru dan vektor teks pelatihan\n",
    "similarity_scoresL = cosine_similarity(new_text_vector, X)\n",
    "predicted_topic_index_cosineL = similarity_scoresL.argmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fungsi ini langsunng menggunakan library `cosine_similarity` dari `sklearn.metrics.pairwise` untuk menghitung `similarity cosine` antara dua vektor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Experiment**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNB atau Multinomial Naive Bayes adalah salah satu bentuk dari algoritma Naive Bayes yang diadaptasi khusus untuk data yang terstruktur dalam format multiclass, seperti teks yang direpresentasikan dalam bentuk vektor. MNB mengasumsikan bahwa setiap fitur (kata) dalam data teks adalah independen, yang berarti bahwa kemunculan satu kata tidak memengaruhi kemunculan kata lainnya. MNB menghitung probabilitas kemunculan setiap kata dalam setiap kelas, dan kemudian mengalikan probabilitas tersebut untuk mendapatkan probabilitas dari seluruh data teks. MNB memilih kelas dengan probabilitas tertinggi sebagai prediksi.\n",
    "\n",
    "* **Keunggulan**: Cepat, sederhana, dan efisien dalam memproses data teks. Toleran terhadap fitur yang jarang atau tidak ada pada dataset pelatihan.\n",
    "* **Keterbatasan**: MNB menganggap setiap fitur (kata) sebagai independen, yang mungkin tidak sesuai dengan keadaan sebenarnya dalam data teks di mana konteks penting. MNB tidak dapat menangani hubungan antara kata-kata dalam data teks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lakukan klasifikasi menggunakan Naive Bayes\n",
    "classifier_nb = MultinomialNB()\n",
    "classifier_nb.fit(X, [0, 1, 2])\n",
    "predicted_topic_index_nb = classifier_nb.predict(new_text_vector)\n",
    "prob_nb = classifier_nb.predict_proba(new_text_vector)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regresi Logistik adalah model statistik yang digunakan untuk klasifikasi. Ini menghitung probabilitas bahwa suatu instance termasuk dalam kelas tertentu berdasarkan fitur-fiturnya. Regresi Logistik menggunakan fungsi logistik untuk menghitung probabilitas, dan kemudian memprediksi kelas dengan probabilitas tertinggi. Regresi Logistik adalah model linier, yang berarti bahwa ia mencari hubungan linier antara fitur dan target. Regresi Logistik adalah model yang sederhana dan efisien, dan sering digunakan sebagai model dasar untuk klasifikasi.\n",
    "\n",
    "* **Keunggulan**: Mudah diinterpretasikan, cocok untuk klasifikasi biner dan multiclass, dan relatif tahan terhadap overfitting dengan parameter yang sesuai.\n",
    "* **Keterbatasan**: Kurang efisien dalam menangani dataset dengan dimensi yang sangat tinggi atau ketika hubungan antara fitur dan target tidak linear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inisialisasi klasifikasi Logistic Regression\n",
    "classifier_lr = LogisticRegression()\n",
    "classifier_lr.fit(X, [0, 1, 2])\n",
    "predicted_topic_index_lr = classifier_lr.predict(new_text_vector)\n",
    "prob_lr = classifier_lr.predict_proba(new_text_vector)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest adalah model ensemble yang terdiri dari sejumlah pohon keputusan. Setiap pohon keputusan dalam Random Forest dibangun secara independen menggunakan sub-klas dataset yang diambil secara acak. Saat melakukan prediksi, setiap pohon memberikan prediksi kelas, dan hasil akhirnya diambil melalui mayoritas suara (untuk klasifikasi) dari semua pohon.\n",
    "\n",
    "* **Keunggulan**: Random Forest efektif dalam menangani dataset dengan fitur yang banyak dan kompleks, toleran terhadap overfitting karena penggunaan ensambel, dan relatif mudah untuk diimplementasikan. Selain itu, Random Forest dapat memberikan perkiraan probabilitas untuk prediksi kelas.\n",
    "* **Keterbatasan**: Meskipun Random Forest cenderung menghasilkan model yang lebih baik daripada pohon keputusan tunggal, kompleksitas modelnya membuatnya lebih sulit untuk diinterpretasi. Selain itu, karena jumlah pohon yang digunakan dapat mempengaruhi kinerja dan waktu pelatihan, ada juga overhead komputasi yang harus dipertimbangkan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "classifier_rf.fit(X, [0, 1, 2])\n",
    "predicted_topic_index_rf = classifier_rf.predict(new_text_vector)\n",
    "prob_rf= classifier_rf.predict_proba(new_text_vector)[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Classification Results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "topik = ['Kesehatan', 'Olahraga', 'Keuangan']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Cosine Similarity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Manual</th>\n",
       "      <th>Library</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Kesehatan</th>\n",
       "      <td>0.016476</td>\n",
       "      <td>0.016476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Olahraga</th>\n",
       "      <td>0.224544</td>\n",
       "      <td>0.224544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Keuangan</th>\n",
       "      <td>0.048536</td>\n",
       "      <td>0.048536</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Manual   Library\n",
       "Kesehatan  0.016476  0.016476\n",
       "Olahraga   0.224544  0.224544\n",
       "Keuangan   0.048536  0.048536"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Buat DataFrame dengan kedua kolom\n",
    "df_similarity_scoresM = pd.DataFrame(similarity_scoresM, columns=['Manual'], index=['Kesehatan', 'Olahraga', 'Keuangan'])\n",
    "df_similarity_scoresL = pd.DataFrame(similarity_scoresL[0], columns=['Library'], index=['Kesehatan', 'Olahraga', 'Keuangan'])\n",
    "df_similarity_scores = pd.concat([df_similarity_scoresM, df_similarity_scoresL], axis=1)\n",
    "\n",
    "df_similarity_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Klasifikasi menggunakan Kesamaan Kosinus Hitung Manual: Olahraga , dengan nilai kemiripan kosinus maksimum: 0.22454435656953595\n",
      "Klasifikasi menggunakan Kesamaan Kosinus dari Scikit-learn: Olahraga , dengan nilai kemiripan kosinus maksimum: 0.22454435656953597\n"
     ]
    }
   ],
   "source": [
    "# Output hasil klasifikasi\n",
    "print(\"Klasifikasi menggunakan Kesamaan Kosinus Hitung Manual:\", topik[predicted_topic_index_cosineM], \", dengan nilai kemiripan kosinus maksimum:\", max(similarity_scoresM))\n",
    "print(\"Klasifikasi menggunakan Kesamaan Kosinus dari Scikit-learn:\", topik[predicted_topic_index_cosineL], \", dengan nilai kemiripan kosinus maksimum:\", similarity_scoresL.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hasil Kesamaan Kosinus new_text dalam setiap topic memiliki nilai yang berbeda-beda. Nilai similarity cosine tertinggi menunjukkan bahwa new_text memiliki kemiripan yang tinggi dengan topik tersebut. Topic Olahraga memiliki nilai similarity cosine sebesar `0.224544`, Topic Kesehatan memiliki nilai similarity cosine sebesar `0.0016476`, dan Topic Keuangan memiliki nilai similarity cosine sebesar `0.048536`. Topic Olahraga memiliki nilai similarity cosine tertinggi, sehingga new_text diprediksi sebagai topik Olahraga."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dapat dilihat pula bahwa tidak ada perbedaan sedikitpun antara hasil `cosine similarity` yang dihitung secara manual dan menggunakan library. Hal ini menunjukkan bahwa kedua metode menghasilkan hasil yang serupa dalam mengukur kemiripan antara dua vektor. Namun, penggunaan library lebih efisien dan mudah digunakan daripada perhitungan manual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Experiment**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Probabilitas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Kesehatan</th>\n",
       "      <td>0.002092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Olahraga</th>\n",
       "      <td>0.991179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Keuangan</th>\n",
       "      <td>0.006729</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Probabilitas\n",
       "Kesehatan      0.002092\n",
       "Olahraga       0.991179\n",
       "Keuangan       0.006729"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Buat DataFrame untuk probabilitas Naive Bayes\n",
    "df_predict_nb = pd.DataFrame(prob_nb, columns=['Probabilitas'], index=['Kesehatan', 'Olahraga', 'Keuangan'])\n",
    "df_predict_nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Klasifikasi menggunakan Naive Bayes: Olahraga , dengan probabilitas: 0.9911794186911391\n"
     ]
    }
   ],
   "source": [
    "print(\"Klasifikasi menggunakan Naive Bayes:\", topik[predicted_topic_index_nb[0]], \", dengan probabilitas:\", prob_nb[predicted_topic_index_nb[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multinomial Naive Bayes memberikan hasil yang baik dalam klasifikasi topik berdasarkan data teks. Dalam eksperimen ini, Multinomial Naive Bayes memberikan probabilitas prediksi sebesar `0.991179` untuk topik Olahraga, `0.002092` untuk topik Kesehatan, dan `0.006729` untuk topik Keuangan. Prediksi topik Olahraga adalah yang tertinggi, sehingga new_text diprediksi sebagai topik Olahraga oleh model Multinomial Naive Bayes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Probabilitas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Kesehatan</th>\n",
       "      <td>0.275674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Olahraga</th>\n",
       "      <td>0.628530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Keuangan</th>\n",
       "      <td>0.095796</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Probabilitas\n",
       "Kesehatan      0.275674\n",
       "Olahraga       0.628530\n",
       "Keuangan       0.095796"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Buat DataFrame untuk probabilitas Logistic Regression\n",
    "df_predict_lr = pd.DataFrame(prob_lr, columns=['Probabilitas'], index=['Kesehatan', 'Olahraga', 'Keuangan'])\n",
    "df_predict_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Klasifikasi menggunakan Logistic Regression: Olahraga , dengan probabilitas: 0.628529996048402\n"
     ]
    }
   ],
   "source": [
    "print(\"Klasifikasi menggunakan Logistic Regression:\", topik[predicted_topic_index_lr[0]], \", dengan probabilitas:\", prob_lr[predicted_topic_index_lr[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression memberikan hasil yang cukup dalam klasifikasi topik berdasarkan data teks. Dalam eksperimen ini, Logistic Regression memberikan probabilitas prediksi sebesar `0.628530` untuk topik Olahraga, `0.275674` untuk topik Kesehatan, dan `0.095796` untuk topik Keuangan. Prediksi topik Olahraga adalah yang tertinggi, sehingga new_text diprediksi sebagai topik Olahraga oleh model Logistic Regression. Probabilitas nya jauh lebih rendah dibandingkan dengan Multinomial Naive Bayes. Hal ini menunjukan performa Multinomial Naive Bayes jauh lebih baik dibandingkan dengan Logistic Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Skor Keputusan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Kesehatan</th>\n",
       "      <td>0.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Olahraga</th>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Keuangan</th>\n",
       "      <td>0.30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Skor Keputusan\n",
       "Kesehatan            0.44\n",
       "Olahraga             0.26\n",
       "Keuangan             0.30"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Buat DataFrame untuk probabilitas Support Vector Machine\n",
    "df_predict_knn = pd.DataFrame(prob_rf, columns=['Probabilitas'], index=['Kesehatan', 'Olahraga', 'Keuangan'])\n",
    "df_predict_knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Klasifikasi menggunakan KNN: Kesehatan , dengan nilai skor keputusan: 0.44\n"
     ]
    }
   ],
   "source": [
    "print(\"Klasifikasi menggunakan KNN:\", topik[predicted_topic_index_rf[0]], \", dengan nilai skor keputusan:\", prob_rf[predicted_topic_index_rf[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Untuk model Random Forest memberikan hasil yang kurang baik dalam klasifikasi topik berdasarkan data teks. Dalam eksperimen ini, Random Forest memberikan probabilitas prediksi sebesar `0.44` untuk topik Olahraga, `0.26` untuk topik Kesehatan, dan `0.30` untuk topik Keuangan. Prediksi topik Kesehatan adalah yang tertinggi, sehingga new_text diprediksi sebagai topik Kesehatan oleh model Random Forest. Hasil klasifikasi yang salah ini menunjukkan bahwa Random Forest tidak cocok untuk dataset ini. Hal ini kemungkinan disebabkan oleh sedikitnya dataset yang digunakan, sehingga Random Forest tidak dapat mempelajari pola yang tepat dalam data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Kesimpulan**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dari hasil eksperimen, dapat disimpulkan bahwa Multinomial Naive Bayes adalah model yang paling baik dalam klasifikasi topik berdasarkan data teks. Multinomial Naive Bayes memberikan probabilitas prediksi tertinggi untuk topik Olahraga, yang sesuai dengan hasil similarity cosine. Logistic Regression memberikan hasil yang cukup baik, namun probabilitas prediksi nya jauh lebih rendah dibandingkan dengan Multinomial Naive Bayes. Random Forest memberikan hasil yang kurang baik, dengan prediksi yang salah. Hal ini menunjukkan bahwa Multinomial Naive Bayes adalah model yang paling cocok untuk dataset ini, dan dapat digunakan untuk klasifikasi topik berdasarkan data teks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
